{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_Encoder.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMLydrMZaneHV1afAWueQkj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or6-qt1ibh96"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "\n",
        "\n",
        "#TransformerClasification層でのclsトークンの変換\n",
        "def LabelCls_convert(CLS):\n",
        "  \n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  m = nn.Softmax(dim=1)\n",
        "  CLS2 = m(CLS)\n",
        "\n",
        "  _, PRED = torch.max(CLS, 1)  # ラベルを予測\n",
        "  a = CLS.dtype\n",
        "  cls_list = []\n",
        "  for i in range(len(PRED)):\n",
        "    if PRED[i] == 1:\n",
        "      cls_list.append(ou_4)\n",
        "    else:\n",
        "      cls_list.append(ou2_4)\n",
        "  cls_numpy = np.array(cls_list)\n",
        "  labelcls = torch.tensor(cls_numpy)\n",
        "  labelcls = labelcls.to(device)\n",
        "  labelcls = labelcls.to(a)\n",
        "\n",
        "  return labelcls, PRED, CLS\n",
        "\n",
        "\n",
        "#########################################\n",
        "#Transformer \n",
        "#######################################\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    '''入力された単語の位置を示すベクトル情報を付加する'''\n",
        "\n",
        "#max_seq_len はパディングしないといけない？\n",
        "    def __init__(self, d_model=768, max_seq_len=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model  # 単語ベクトルの次元数\n",
        "\n",
        "        # 単語の順番（pos）と埋め込みベクトルの次元の位置（i）によって一意に定まる値の表をpeとして作成\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        #print(pe)\n",
        "\n",
        "        # GPUが使える場合はGPUへ送る\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        pe = pe.to(device)\n",
        "\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos /\n",
        "                                          (10000 ** ((2 * (i + 1))/d_model)))\n",
        "\n",
        "        # 表peの先頭に、ミニバッチ次元となる次元を足す\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "        # 勾配を計算しないようにする\n",
        "        self.pe.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 入力xとPositonal Encodingを足し算する\n",
        "        # xがpeよりも小さいので、大きくする\n",
        "        ret = math.sqrt(self.d_model)*x + self.pe\n",
        "        return ret\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    '''シングルAttentionで実装'''\n",
        "\n",
        "    def __init__(self, d_model=768):\n",
        "        super().__init__()\n",
        "\n",
        "        # SAGANでは1dConvを使用したが、今回は全結合層で特徴量を変換する\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # 出力時に使用する全結合層\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Attentionの大きさ調整の変数\n",
        "        self.d_k = d_model\n",
        "\n",
        "    def forward(self, cls_tokens, pe):\n",
        "      #mask = torch.zeros(768)\n",
        "      #for i in range(0, len(cls_tokens)):\n",
        "        \n",
        "        # 全結合層で特徴量を変換\n",
        "        k = self.k_linear(pe)\n",
        "        q = self.q_linear(cls_tokens)\n",
        "        v = self.v_linear(pe)\n",
        "\n",
        "        # Attentionの値を計算する\n",
        "        # 各値を足し算すると大きくなりすぎるので、root(d_k)で割って調整\n",
        "        \n",
        "        #attention_weights = torch.matmul(q, k.transpose(2, 1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        #attention_weights = torch.matmul(q.unsqueeze(1), k.transpose(2, 1) )/ math.sqrt(self.d_k)\n",
        "        attention_weights = torch.matmul(q, k.transpose(2, 1) )/ math.sqrt(self.d_k)\n",
        "\n",
        "\n",
        "        # ここでmaskを計算\n",
        "        #mask = mask.unsqueeze(1)\n",
        "        #attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # softmaxで規格化をする\n",
        "        normlized_weights = F.softmax(attention_weights, dim=-1)\n",
        "\n",
        "        # AttentionをValueとかけ算\n",
        "        output = torch.matmul(normlized_weights, v)\n",
        "\n",
        "        # 全結合層で特徴量を変換\n",
        "        output = self.out(output)\n",
        "        \n",
        "        return output, normlized_weights\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, classes, num_heads):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    self.head_dim = d_model // num_heads\n",
        "    self.classes = classes\n",
        "   \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = nn.Linear(d_model, d_model)\n",
        "    self.wk = nn.Linear(d_model, d_model)\n",
        "    self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.d_k = d_model\n",
        "\n",
        "    # 出力時に使用する全結合層\n",
        "    self.out = nn.Linear(d_model, d_model)\n",
        "    self._reset_parameters()\n",
        "        \n",
        " ## def split_heads(self, x, batch_size):\n",
        "    \"\"\"最後の次元を(num_heads, depth)に分割。\n",
        "    結果をshapeが(batch_size, num_heads, seq_len, depth)となるようにリシェイプする。\n",
        "    \"\"\"\n",
        "  ##  x = torch.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        " ##   return torch.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def _reset_parameters(self):\n",
        "    # Original Transformer initialization, see PyTorch documentation\n",
        "    nn.init.xavier_uniform_(self.wq.weight)\n",
        "    self.wq.bias.data.fill_(0)\n",
        "\n",
        "    nn.init.xavier_uniform_(self.wk.weight)\n",
        "    self.wk.bias.data.fill_(0)\n",
        "\n",
        "    nn.init.xavier_uniform_(self.wv.weight)\n",
        "    self.wv.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "  def forward(self, cls_tokens, pe):\n",
        "    #batch_sizeの指定\n",
        "    batch_size = 4\n",
        "    \n",
        "    q = self.wq(cls_tokens)\n",
        "    k = self.wk(pe)\n",
        "    v = self.wv(pe)\n",
        "    d_k = self.d_k\n",
        "\n",
        "\n",
        "    q = q.reshape( batch_size, self.classes,self.num_heads, self.head_dim)\n",
        "    k = k.reshape(batch_size, self.classes, self.num_heads, self.head_dim)\n",
        "    v = v.reshape(batch_size, self.classes, self.num_heads, self.head_dim)\n",
        "\n",
        "\n",
        "    #q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    #k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    #v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    \n",
        "    #attention_weights = torch.matmul(q, k.transpose(-2, -1))\n",
        "    #attention_weights = attention_weights / math.sqrt(d_k)\n",
        "\n",
        "    #attention_weights = torch.matmul(q.unsqueeze(1), k.transpose(-2, -1) )/ math.sqrt(self.d_k)\n",
        "    attention_weights = torch.matmul(q, k.transpose(3, 2) )/ math.sqrt(self.d_k)  \n",
        "    \n",
        "    # ここでmaskを計算\n",
        "    #mask = mask.unsqueeze(1)\n",
        "    #attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n",
        "    # softmaxで規格化をする\n",
        "    \n",
        "    normlized_weights = F.softmax(attention_weights, dim=-1)\n",
        "    # AttentionをValueとかけ算\n",
        "    values = torch.matmul(normlized_weights, v)\n",
        "    # 全結合層で特徴量を変換\n",
        "\n",
        "    values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "    values = values.reshape(batch_size, self.classes, self.d_model)\n",
        "    output = self.out(values)\n",
        "    \n",
        "    return output, normlized_weights\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model = 768, d_ff=1536, dropout=0.1):\n",
        "        '''Attention層から出力を単純に全結合層2つで特徴量を変換するだけのユニットです'''\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_1(x)\n",
        "        x = self.dropout(F.relu(x))\n",
        "        x = self.linear_2(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model=768, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # LayerNormalization層\n",
        "        self.norm_1 = nn.LayerNorm(d_model)\n",
        "        self.norm_2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Attention層\n",
        "        #self.attn = Attention(d_model=d_model)\n",
        "        self.attn = MultiHeadAttention(d_model=768, classes=7,  num_heads=4)\n",
        "\n",
        "        # Attentionのあとの全結合層2つ\n",
        "        self.ff = FeedForward(d_model)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, pn, x):\n",
        "        # PositionalEncoding を行った後に正規化する\n",
        "        x_normlized = self.norm_1(x)\n",
        "\n",
        "        output, normlized_weights = self.attn(pn, x_normlized)\n",
        "        #FeedForward層の入力作成\n",
        "        x2 = x + self.dropout_1(output)\n",
        "\n",
        "        # 正規化と全結合層\n",
        "        x_normlized2 = self.norm_2(x2)\n",
        "        output = x2 + self.dropout_2(self.ff(x_normlized2))\n",
        "\n",
        "\n",
        "        return output, normlized_weights\n",
        "\n",
        "class ClassificationHead(nn.Module):\n",
        "    '''Transformer_Blockの出力を使用し、最後にクラス分類させる'''\n",
        "\n",
        "    def __init__(self, d_model=768, output_dim=14):\n",
        "        super().__init__()\n",
        "\n",
        "        # 全結合層\n",
        "        self.linear = nn.Linear(d_model, output_dim)  # output_dimはポジ・ネガの2つ\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.linear.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "      #バッチサイズに合わせて変化\n",
        "        x0 = torch.stack([torch.mean(x[0,:,:],0), torch.mean(x[1,:,:], 0), torch.mean(x[2,:,:],0), torch.mean(x[3,:,:], 0)], 0)\n",
        "        #x0 = x[:, 0, :]  # 各ミニバッチの各文の cls の特徴量（768次元）を取り出す\n",
        "        out = self.linear(x0)\n",
        "    \n",
        "    # 14 次元に圧縮せずに, 768 次元のマルチラベル分類特化 CLS を獲得\n",
        "        return out\n",
        "        \n",
        "# 最終的なTransformerモデルのクラス\n",
        "\n",
        "class TransformerClassification(nn.Module):\n",
        "    '''Transformerでクラス分類させる'''\n",
        "\n",
        "    def __init__(self,  d_model=768, max_seq_len=100, output_dim=14):\n",
        "        super().__init__()\n",
        "\n",
        "        # モデル構築\n",
        "        \n",
        "        #self.net1 = BERT_net\n",
        "        self.net2 = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n",
        "        self.net3_1 = TransformerBlock(d_model=d_model)\n",
        "        self.net3_2 = TransformerBlock(d_model=d_model)\n",
        "        self.net4 = ClassificationHead(d_model=d_model, output_dim=output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, bert_cls1, x1, bert_cls2, x2, bert_cls3, x3, bert_cls4, x4, bert_cls5, x5, bert_cls6, x6, bert_cls7, x7):\n",
        "\n",
        "        q = torch.stack([x1, x2,  x3,  x4,  x5,  x6,  x7], dim=1)\n",
        "        k_v = torch.stack([bert_cls1, bert_cls2,  bert_cls3,  bert_cls4,  bert_cls5,  bert_cls6,  bert_cls7], dim=1) \n",
        "        \n",
        "        #bert_cls, bert_atten, cls_tok,  x1 = self.net1(text)\n",
        "\n",
        "        #LABEL_CLS, BERT_PRED = LabelCls_convert(bert_cls)\n",
        "\n",
        "        x13, normlized_weights_1 = self.net3_1(q, k_v)  # Self-Attentionで特徴量を変換\n",
        "        #x3_1_2 = x13[:, 0, :]\n",
        "\n",
        "        x3_2, normlized_weights_2 = self.net3_2(q, x13)  # Self-Attentionで特徴量を変換        \n",
        "        tr_cls1 = self.net4(x3_2)  #768次元を14次元に圧縮\n",
        "\n",
        "\n",
        "        #OO = nn.Sigmoid()\n",
        "        return tr_cls1\n",
        "      \n",
        "        #return OO(total_cls)  #normlized_weights_1, #normlized_weights_2"
      ]
    }
  ]
}