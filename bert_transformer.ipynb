{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert-transformer.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN8qqBuQR9HGvdfsa7ORU65",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuuki-kusumoto/kusumoto/blob/main/bert_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg_QR7wj4RgJ",
        "outputId": "486a42b5-7c72-42a6-8a79-14de8056d0f0"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov  8 18:49:40 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bmV6oAa4O0l"
      },
      "source": [
        "!pip install transformers\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install MeCab\n",
        "!pip install mecab-python3\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Ub_dJc4LHe"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n",
        "path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
        "                           shell=True).communicate()[0]).decode('utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsErmYW_4IjO"
      },
      "source": [
        "!pip install ipadic\n",
        "!pip install mecab-python3\n",
        "!pip install unidic-lite\n",
        "# MeCabとtransformersを用意する\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "# 以下で報告があるようにmecab-python3のバージョンを0.996.5にしないとtokezerで落ちる\n",
        "# https://stackoverflow.com/questions/62860717/huggingface-for-japanese-tokenizer\n",
        "!pip install mecab-python3==0.996.5\n",
        "!pip install unidic-lite # これないとMeCab実行時にエラーで落ちる\n",
        "!pip install transformers\n",
        "!pip install \"transformers==2.5.1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16LyvC9N4FeZ"
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers.modeling_bert import BertModel\n",
        "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "\n",
        "# パッケージのimport\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "\n",
        "# 必要なパッケージのimport\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "\n",
        "\n",
        "! curl http://www.cl.ecei.tohoku.ac.jp/resources/sent_lex/pn.csv.m3.120408.trim > pn.csv\n",
        "\n",
        "\n",
        "model = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
        "\n",
        "# model_nameはここから取得(cf. https://huggingface.co/transformers/pretrained_models.html)\n",
        "model_name = \"cl-tohoku/bert-base-japanese\"\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT0s0eAF4B4p"
      },
      "source": [
        "# ファイルをオープンして、1行ずつその内容を読み込んで処理する\n",
        "with open('train_data.txt') as f:\n",
        "    for line in f:\n",
        "        line = line.rstrip()  # 読み込んだ行の末尾には改行文字があるので削除\n",
        "        print(line)\n",
        "        \n",
        "with open('train_data.txt') as f:\n",
        "  texts = f.read()\n",
        "\n",
        "#訓練データをリストにして開く\n",
        "\n",
        "with open('train_data.txt', 'r') as f:\n",
        "    tr_list = f.read().split(\"\\n\")\n",
        "\n",
        "#訓練データをラベルと文章に分けてリストに入れる。\n",
        "\n",
        "tr_word = []\n",
        "tr_label = []\n",
        "\n",
        "for i in range(0,4000):\n",
        "  a = tr_list[i]\n",
        "  b = a.replace('\\t','')\n",
        "\n",
        "  tr_word.append(b[1:len(b)])\n",
        "  tr_label.append(b[0])\n",
        "\n",
        "print(tr_word)\n",
        "print(tr_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTa9WGX23-Rd"
      },
      "source": [
        "with open('test_data.txt') as f:\n",
        "  texts = f.read()\n",
        "  \n",
        "#テストデータをリストにして開く\n",
        "\n",
        "with open('test_data.txt', 'r') as f:\n",
        "    ts_list = f.read().split(\"\\n\")\n",
        "\n",
        "#テストデータをラベルと文章に分けてリストに入れる。\n",
        "\n",
        "ts_word = []\n",
        "ts_label = []\n",
        "\n",
        "for i in range(0,2000):\n",
        "  a = ts_list[i]\n",
        "  b = a.replace('\\t','')\n",
        "\n",
        "  ts_word.append(b[1:len(b)])\n",
        "  ts_label.append(b[0])\n",
        "\n",
        "print(ts_word)\n",
        "print(ts_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xtbfLoJ3tt8"
      },
      "source": [
        "#フレームに\n",
        "import pandas as pd\n",
        "\n",
        "#text, indexの順番にする\n",
        "tr_df1 = pd.DataFrame({'tr_word' : tr_word, 'tr_label' : tr_label})\n",
        "print(len(tr_df1))\n",
        "\n",
        "#フレームに\n",
        "import pandas as pd\n",
        "\n",
        "ts_df = pd.DataFrame({'tr_word' : ts_word, 'tr_label' : ts_label})\n",
        "print(len(ts_df))\n",
        "\n",
        "tr_df = pd.concat([tr_df1, ts_df])\n",
        "print(len(tr_df))\n",
        "tr_df = tr_df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "tr_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHfgICQv3tH9"
      },
      "source": [
        "# tsvファイルで保存する\n",
        "#tr_df = tr_df[:100]\n",
        "# 全体の2割の文章数\n",
        "len_0_2 = len(tr_df) // 5\n",
        "\n",
        "# 前から2割をテストデータとする\n",
        "tr_df[:len_0_2].to_csv(\"./test.tsv\", sep='\\t', index=False, header=None)\n",
        "print(tr_df[:len_0_2].shape)\n",
        "\n",
        "# 前2割からを訓練&検証データとする\n",
        "tr_df[len_0_2:].to_csv(\"./train.tsv\", sep='\\t', index=False, header=None)\n",
        "print(tr_df[len_0_2:].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBfqjesj3qr1"
      },
      "source": [
        "\n",
        "# torchtext.data.Datasetのsplit関数で訓練データと検証データを分ける\n",
        "# train_eval：4000個、test：2000個\n",
        "\n",
        "dataset_train, dataset_eval = dataset_train_eval.split(\n",
        "    split_ratio = 1 - 0.25, random_state=random.seed(1234))\n",
        "\n",
        "# datasetの長さを確認してみる\n",
        "print(dataset_train.__len__())\n",
        "print(dataset_eval.__len__())\n",
        "print(dataset_test.__len__())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeCAMCgM3n_N"
      },
      "source": [
        "# 乱数シードの固定\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED_VALUE = 1234  # これはなんでも良い\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)\n",
        "torch.manual_seed(SEED_VALUE)  # PyTorchを使う場合\n",
        "\n",
        "# torchtext.data.Datasetのsplit関数で訓練データと検証データを分ける\n",
        "# train_eval：4000個、test：2000個\n",
        "\n",
        "dataset_train, dataset_eval = dataset_train_eval.split(\n",
        "    split_ratio = 1 - 0.25, random_state=random.seed(1234))\n",
        "\n",
        "# datasetの長さを確認してみる\n",
        "print(dataset_train.__len__())\n",
        "print(dataset_eval.__len__())\n",
        "print(dataset_test.__len__())\n",
        "\n",
        "# datasetの中身を確認してみる\n",
        "item = next(iter(dataset_train))\n",
        "print(item.Text)\n",
        "print(\"長さ：\", len(item.Text))  # 長さを確認 [CLS]から始まり[SEP]で終わる。512より長いと後ろが切れる\n",
        "print(\"ラベル：\", item.Label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eseqw553kj1"
      },
      "source": [
        "# DataLoaderを作成\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "dl_train = torchtext.legacy.data.Iterator(\n",
        "    dataset_train, batch_size=batch_size, train=True)\n",
        "\n",
        "dl_eval = torchtext.legacy.data.Iterator(\n",
        "    dataset_eval, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "dl_test = torchtext.legacy.data.Iterator(\n",
        "    dataset_test, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "# 辞書オブジェクトにまとめる\n",
        "dataloaders_dict = {\"train\": dl_train, \"val\": dl_eval}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynEpre9Z3hJd"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class BertForReview(nn.Module):\n",
        "    '''BERTモデルにレビュー文の2クラスを判定する部分をつなげたモデル'''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BertForReview, self).__init__()\n",
        "\n",
        "        # BERTモジュール\n",
        "        self.bert = model  # 日本語学習済みのBERTモデル\n",
        "\n",
        "        # headにポジネガ予測を追加\n",
        "        # 入力はBERTの出力特徴量の次元768、出力は14クラス\n",
        "        #cls層の追加\n",
        "        self.cls = nn.Linear(in_features=768, out_features=2)\n",
        "\n",
        "        \n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.cls.weight, std=0.02)\n",
        "        nn.init.normal_(self.cls.bias, 0)\n",
        "    \n",
        "    \n",
        "    def forward(self, input_ids):\n",
        "        '''\n",
        "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
        "        '''\n",
        "\n",
        "        # BERTの基本モデル部分の順伝搬\n",
        "        # 順伝搬させる\n",
        "        #print(\"input_ids :\", input_ids)\n",
        "        result = self.bert(input_ids)  # reult は、sequence_output, pooled_output\n",
        "\n",
        "        # sequence_outputの先頭の単語ベクトルを抜き出す\n",
        "        vec_0 = result[0]  # 最初の0がsequence_outputを示す\n",
        "        vec_02 = vec_0[:, 0, :]  # 全バッチ。先頭0番目の単語の全768要素。　つまりclsトークンを獲得している。\n",
        "        vec_02 = vec_02.view(-1, 768)  # sizeを[batch_size, hidden_size]\n",
        "        vec_03 = vec_0[:, 0:512, :]\n",
        "        output = self.cls(vec_02)  # 全結合層\n",
        "        m = nn.Sigmoid()\n",
        "        out = m(output)\n",
        "\n",
        "        return vec_02, vec_03\n",
        "\n",
        "#モデル構築\n",
        "BERT_net = BertForReview()\n",
        "\n",
        "print('ネットワーク設定完了')\n",
        "\n",
        "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
        "\n",
        "# 1. まず全部を、勾配計算Falseにしてしまう\n",
        "for param in BERT_net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
        "for param in BERT_net.bert.encoder.layer[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 3. 識別器を勾配計算ありに変更\n",
        "for param in BERT_net.cls.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    '''入力された単語の位置を示すベクトル情報を付加する'''\n",
        "\n",
        "#max_seq_len はパディングしないといけない？\n",
        "    def __init__(self, d_model=768, max_seq_len=512):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model  # 単語ベクトルの次元数\n",
        "\n",
        "        # 単語の順番（pos）と埋め込みベクトルの次元の位置（i）によって一意に定まる値の表をpeとして作成\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        #print(pe)\n",
        "\n",
        "        # GPUが使える場合はGPUへ送る\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        pe = pe.to(device)\n",
        "\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos /\n",
        "                                          (10000 ** ((2 * (i + 1))/d_model)))\n",
        "\n",
        "        # 表peの先頭に、ミニバッチ次元となる次元を足す\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "        # 勾配を計算しないようにする\n",
        "        self.pe.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 入力xとPositonal Encodingを足し算する\n",
        "        # xがpeよりも小さいので、大きくする\n",
        "        ret = math.sqrt(self.d_model)*x + self.pe\n",
        "        return ret\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    '''シングルAttentionで実装'''\n",
        "\n",
        "    def __init__(self, d_model=768):\n",
        "        super().__init__()\n",
        "\n",
        "        # SAGANでは1dConvを使用したが、今回は全結合層で特徴量を変換する\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # 出力時に使用する全結合層\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Attentionの大きさ調整の変数\n",
        "        self.d_k = d_model\n",
        "\n",
        "    def forward(self, cls_tokens, pe):\n",
        "      #mask = torch.zeros(768)\n",
        "      #for i in range(0, len(cls_tokens)):\n",
        "        \n",
        "        # 全結合層で特徴量を変換\n",
        "        k = self.k_linear(pe)\n",
        "        q = self.q_linear(cls_tokens)\n",
        "        v = self.v_linear(pe)\n",
        "\n",
        "        # Attentionの値を計算する\n",
        "        # 各値を足し算すると大きくなりすぎるので、root(d_k)で割って調整\n",
        "        \n",
        "        #attention_weights = torch.matmul(q, k.transpose(2, 1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        attention_weights = torch.matmul(q.unsqueeze(1), k.transpose(2, 1) )/ math.sqrt(self.d_k)\n",
        "\n",
        "\n",
        "\n",
        "        # ここでmaskを計算\n",
        "        #mask = mask.unsqueeze(1)\n",
        "        #attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # softmaxで規格化をする\n",
        "        normlized_weights = F.softmax(attention_weights, dim=-1)\n",
        "\n",
        "        # AttentionをValueとかけ算\n",
        "        output = torch.matmul(normlized_weights, v)\n",
        "\n",
        "        # 全結合層で特徴量を変換\n",
        "        output = self.out(output)\n",
        "        \n",
        "\n",
        "        return output, normlized_weights\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model = 768, d_ff=1024, dropout=0.1):\n",
        "        '''Attention層から出力を単純に全結合層2つで特徴量を変換するだけのユニットです'''\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_1(x)\n",
        "        x = self.dropout(F.relu(x))\n",
        "        x = self.linear_2(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model=768, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # LayerNormalization層\n",
        "        self.norm_1 = nn.LayerNorm(d_model)\n",
        "        self.norm_2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Attention層\n",
        "        self.attn = Attention(d_model)\n",
        "\n",
        "        # Attentionのあとの全結合層2つ\n",
        "        self.ff = FeedForward(d_model)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, pn, x):\n",
        "        # PositionalEncoding を行った後に正規化する\n",
        "        x_normlized = self.norm_1(x)\n",
        "\n",
        "        output, normlized_weights = self.attn(pn, x_normlized)\n",
        "        #FeedForward層の入力作成\n",
        "        x2 = x + self.dropout_1(output)\n",
        "\n",
        "        # 正規化と全結合層\n",
        "        x_normlized2 = self.norm_2(x2)\n",
        "        output = x2 + self.dropout_2(self.ff(x_normlized2))\n",
        "\n",
        "        return output, normlized_weights\n",
        "\n",
        "class ClassificationHead(nn.Module):\n",
        "    '''Transformer_Blockの出力を使用し、最後にクラス分類させる'''\n",
        "\n",
        "    def __init__(self, d_model=768, output_dim=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # 全結合層\n",
        "        self.linear = nn.Linear(d_model, output_dim)  # output_dimはポジ・ネガの2つ\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.linear.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = x[:, 0, :]  # 各ミニバッチの各文の cls の特徴量（768次元）を取り出す\n",
        "        out = self.linear(x0)\n",
        "\n",
        "        return out\n",
        "        \n",
        "# 最終的なTransformerモデルのクラス\n",
        "\n",
        "\n",
        "class TransformerClassification(nn.Module):\n",
        "    '''Transformerでクラス分類させる'''\n",
        "\n",
        "    def __init__(self,  d_model=768, max_seq_len=512, output_dim=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # モデル構築\n",
        "        \n",
        "        self.net1 = BERT_net\n",
        "        self.net2 = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n",
        "        self.net3_1 = TransformerBlock(d_model=d_model)\n",
        "        #self.net3_2 = TransformerBlock(d_model=d_model)\n",
        "        self.net4 = ClassificationHead(output_dim=output_dim, d_model=d_model)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        cls, x1 = self.net1(text)\n",
        "\n",
        "        x2 = self.net2(x1)  # Positon情報を足し算\n",
        "        \n",
        "        x3_1, normlized_weights_1 = self.net3_1(cls, x2)  # Self-Attentionで特徴量を変換\n",
        "        \n",
        "        #x3_2, normlized_weights_2 = self.net3_2(x3_1, mask)  # Self-Attentionで特徴量を変換\n",
        "        \n",
        "        x4 = self.net4(x3_1)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n",
        "        \n",
        "        return x4, normlized_weights_1 #, normlized_weights_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVmpZadm3eQ6"
      },
      "source": [
        "# モデル構築\n",
        "TC_net = TransformerClassification()\n",
        "    \n",
        "# ネットワークの初期化を定義\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        # Liner層の初期化\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "# 訓練モードに設定\n",
        "TC_net.train()\n",
        "\n",
        "# TransformerBlockモジュールを初期化実行\n",
        "TC_net.net3_1.apply(weights_init)\n",
        "#TC_net.net3_2.apply(weights_init)\n",
        "print('ネットワーク設定完了')\n",
        "\n",
        "# 損失関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算\n",
        "\n",
        "# 最適化手法の設定\n",
        "learning_rate = 2e-5\n",
        "optimizer = optim.Adam(TC_net.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4G__j-N3ZYm"
      },
      "source": [
        "# モデルを学習させる関数を作成\n",
        "\n",
        "def train_model(tc_net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "    # ネットワークをGPUへ\n",
        "    #bert_net.to(device)\n",
        "    tc_net.to(device)\n",
        "\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "\n",
        "    acc_train_list = []\n",
        "    acc_val_list = []\n",
        "\n",
        "\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "\n",
        "            if phase == 'train':\n",
        "                #bert_net.train()  # モデルを訓練モードに\n",
        "                tc_net.train()\n",
        "            else:\n",
        "                #bert_net.eval()   # モデルを検証モードに\n",
        "                tc_net.eval()\n",
        "\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書オブジェクト\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                inputs = batch.Text[0].to(device)  # 文章\n",
        "                labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                #BERTに入力\n",
        "                #cls_tokens, bert_tokens  = bert_net(inputs)\n",
        "\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "\n",
        "                    # Transformerに入力\n",
        "                    outputs, weights  = tc_net(inputs)\n",
        "                    \n",
        "                    loss = criterion(outputs, labels)  # 損失を計算\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)  # ラベルを予測（dim=1 列方向のＭａｘを取得、predsは最大のindex）\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()   #損失の計算\n",
        "                        optimizer.step()  # 勾配の更新\n",
        "\n",
        "\n",
        "                    # 結果の計算\n",
        "                    epoch_loss += loss.item() * inputs.size(0)  # lossの合計を更新\n",
        "                    # 正解数の合計を更新\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double(\n",
        "            ) / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            if phase == 'train':\n",
        "              train_loss = np.append(train_loss, epoch_loss)\n",
        "              acc_train_list = np.append(acc_train_list, epoch_acc.cpu().numpy())\n",
        "\n",
        "            else:\n",
        "              val_loss = np.append(val_loss, epoch_loss)\n",
        "              acc_val_list = np.append(acc_val_list, epoch_acc.cpu().numpy())\n",
        "\n",
        "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
        "                                                                           phase, epoch_loss, epoch_acc))\n",
        "\n",
        "\n",
        "    return tc_net, train_loss, val_loss, acc_train_list, acc_val_list\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}