{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMv3R6iHSMlgAtGvwJ6A8N/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuuki-kusumoto/kusumoto/blob/main/ksumoto_BERT_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itlCHq-GNqNd"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install MeCab\n",
        "!pip install mecab-python3\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "!pip install ipadic\n",
        "!pip install mecab-python3\n",
        "!pip install unidic-lite\n",
        "# MeCabとtransformersを用意する\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "# 以下で報告があるようにmecab-python3のバージョンを0.996.5にしないとtokezerで落ちる\n",
        "# https://stackoverflow.com/questions/62860717/huggingface-for-japanese-tokenizer\n",
        "!pip install mecab-python3==0.996.5\n",
        "!pip install unidic-lite # これないとMeCab実行時にエラーで落ちる\n",
        "!pip install transformers\n",
        "!pip install \"transformers==2.5.1\"\n",
        "!pip install \"torchtext== 0.11\"\n",
        "\n",
        "import subprocess\n",
        "\n",
        "cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n",
        "path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
        "                           shell=True).communicate()[0]).decode('utf-8')\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers.modeling_bert import BertModel\n",
        "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "# パッケージのimport\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "\n",
        "# 必要なパッケージのimport\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "\n",
        "\n",
        "! curl http://www.cl.ecei.tohoku.ac.jp/resources/sent_lex/pn.csv.m3.120408.trim > pn.csv\n",
        "\n",
        "\n",
        "model = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
        "\n",
        "# model_nameはここから取得(cf. https://huggingface.co/transformers/pretrained_models.html)\n",
        "model_name = \"cl-tohoku/bert-base-japanese\"\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTのソースコード"
      ],
      "metadata": {
        "id": "IQ3SJPD-N1l0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class BertForReview(nn.Module):\n",
        "    '''BERTモデルにレビュー文の2クラスを判定する部分をつなげたモデル'''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BertForReview, self).__init__()\n",
        "\n",
        "        # BERTモジュール\n",
        "        #self.bert = model  # 日本語学習済みのBERTモデル\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking',\n",
        "                                              output_attentions=True,\n",
        "                                              output_hidden_states=True)\n",
        "\n",
        "        # headにポジネガ予測を追加\n",
        "        # 入力はBERTの出力特徴量の次元768、出力は14クラス\n",
        "        #cls層の追加\n",
        "        self.cls = nn.Linear(in_features=768, out_features=2)\n",
        "        \n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.cls.weight, std=0.02)\n",
        "        nn.init.normal_(self.cls.bias, 0)    \n",
        "    \n",
        "    def forward(self, input_ids):\n",
        "        '''\n",
        "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
        "        '''\n",
        "\n",
        "        # BERTの基本モデル部分の順伝搬\n",
        "        # 順伝搬させる\n",
        "        #print(\"input_ids :\", input_ids)\n",
        "        result = self.bert(input_ids)  # reult は、sequence_output, pooled_output\n",
        "        #result, _, attentions = self.bert(input_ids, output_attentions=True)\n",
        "\n",
        "        all_attentions = result[2]\n",
        "        last_layer_attention = all_attentions[-1]\n",
        "        hidden_states = result[3]\n",
        "\n",
        "        m = nn.Softmax(dim=1)\n",
        "\n",
        "        #vec1 = self._get_cls_vec(hidden_states[-1])\n",
        "        # sequence_outputの先頭の単語ベクトルを抜き出す\n",
        "        vec_0 = result[0]  # 最初の0がsequence_outputを示す\n",
        "        vec_02 = vec_0[:, 0, :]  # 全バッチ。先頭0番目の単語の全768要素。　つまりclsトークンを獲得している。\n",
        "        vec_03 = vec_02.view(-1, 768)  # sizeを[batch_size, hidden_size] clsトークン\n",
        "        #vec_03 = vec_0[:, 0:100, :] #文章の埋め込み表現\n",
        "        output = self.cls(vec_03)  # 全結合層\n",
        "        A = F.log_softmax(output, dim=1)\n",
        "        B = m(output)\n",
        "\n",
        "        #return vec_02, vec_03, last_layer_attention,hidden_states  #vec_02 : clsトークン, #vec_03 : 文章の特徴量\n",
        "        \n",
        "        return m(output), all_attentions, vec_02#, vec_03\n",
        "\n",
        "\n",
        "# モデル構築\n",
        "BERT_net = BertForReview()\n",
        "\n",
        "# 訓練モードに設定\n",
        "BERT_net.train()\n",
        "\n",
        "print('ネットワーク設定完了')\n",
        "\n",
        "\n",
        "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
        "\n",
        "# 1. まず全部を、勾配計算Falseにしてしまう\n",
        "for param in BERT_net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
        "for param in BERT_net.bert.encoder.layer[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 3. 識別器を勾配計算ありに変更\n",
        "for param in BERT_net.cls.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# BERTの元の部分はファインチューニング\n",
        "bert_optimizer = optim.Adam([\n",
        "    {'params': BERT_net.bert.encoder.layer[-1].parameters(), 'lr': 1e-4},\n",
        "    {'params': BERT_net.cls.parameters(), 'lr': 1e-3}\n",
        "])\n",
        "\n",
        "# 損失関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "iEiINiu-N0uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Encoder のソースコード"
      ],
      "metadata": {
        "id": "932AjWz8ObL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "\n",
        "a_po = \"朝食ポジティブ\"\n",
        "a_po = tokenizer.encode(a_po)\n",
        "#print(a_po)\n",
        "a_po_to = torch.LongTensor(a_po).unsqueeze(0)\n",
        "#print(a_po_to)\n",
        "#clsトークン\n",
        "ou = model(a_po_to)\n",
        "ou_0 = ou[0]\n",
        "ou_1 = ou_0[:, 0, :]\n",
        "ou_2 = ou_1.view(-1, 768)\n",
        "ou_3 = ou_2[0]\n",
        "\n",
        "\n",
        "a_ne = \"朝食ネガティブ\"\n",
        "a_ne = tokenizer.encode(a_ne)\n",
        "a_ne_to = torch.LongTensor(a_ne).unsqueeze(0)\n",
        "#clsトークン\n",
        "ou2 = model(a_ne_to)\n",
        "ou2_0 = ou2[0]\n",
        "ou2_1 = ou2_0[:, 0, :]\n",
        "ou2_2 = ou2_1.view(-1, 768)\n",
        "ou2_3 = ou2_2[0]\n",
        "\n",
        "ou_4 = ou_3.tolist()\n",
        "ou2_4 = ou2_3.tolist()\n",
        "\n",
        "#TransformerClasification層でのclsトークンの変換\n",
        "def LabelCls_convert(CLS):\n",
        "  \n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  m = nn.Softmax(dim=1)\n",
        "  CLS2 = m(CLS)\n",
        "\n",
        "  _, PRED = torch.max(CLS, 1)  # ラベルを予測\n",
        "  a = CLS.dtype\n",
        "  cls_list = []\n",
        "  for i in range(len(PRED)):\n",
        "    if PRED[i] == 1:\n",
        "      cls_list.append(ou_4)\n",
        "    else:\n",
        "      cls_list.append(ou2_4)\n",
        "  cls_numpy = np.array(cls_list)\n",
        "  labelcls = torch.tensor(cls_numpy)\n",
        "  labelcls = labelcls.to(device)\n",
        "  labelcls = labelcls.to(a)\n",
        "\n",
        "  return labelcls, PRED, CLS\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    '''入力された単語の位置を示すベクトル情報を付加する'''\n",
        "\n",
        "#max_seq_len はパディングしないといけない？\n",
        "    def __init__(self, d_model=768, max_seq_len=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model  # 単語ベクトルの次元数\n",
        "\n",
        "        # 単語の順番（pos）と埋め込みベクトルの次元の位置（i）によって一意に定まる値の表をpeとして作成\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        #print(pe)\n",
        "\n",
        "        # GPUが使える場合はGPUへ送る\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        pe = pe.to(device)\n",
        "\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos /\n",
        "                                          (10000 ** ((2 * (i + 1))/d_model)))\n",
        "\n",
        "        # 表peの先頭に、ミニバッチ次元となる次元を足す\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "        # 勾配を計算しないようにする\n",
        "        self.pe.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 入力xとPositonal Encodingを足し算する\n",
        "        # xがpeよりも小さいので、大きくする\n",
        "        ret = math.sqrt(self.d_model)*x + self.pe\n",
        "        return ret\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    '''シングルAttentionで実装'''\n",
        "\n",
        "    def __init__(self, d_model=768):\n",
        "        super().__init__()\n",
        "\n",
        "        # SAGANでは1dConvを使用したが、今回は全結合層で特徴量を変換する\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # 出力時に使用する全結合層\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Attentionの大きさ調整の変数\n",
        "        self.d_k = d_model\n",
        "\n",
        "    def forward(self, pe):\n",
        "      #mask = torch.zeros(768)\n",
        "      #for i in range(0, len(cls_tokens)):\n",
        "        \n",
        "        # 全結合層で特徴量を変換\n",
        "        k = self.k_linear(pe)\n",
        "        q = self.q_linear(pe)\n",
        "        v = self.v_linear(pe)\n",
        "\n",
        "        print(k)\n",
        "        print(\"AAAAAAAAAAA\", k.shape)\n",
        "\n",
        "        # Attentionの値を計算する\n",
        "        # 各値を足し算すると大きくなりすぎるので、root(d_k)で割って調整\n",
        "        \n",
        "        #attention_weights = torch.matmul(q, k.transpose(2, 1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        #attention_weights = torch.matmul(q.unsqueeze(1), k.transpose(2, 1) )/ math.sqrt(self.d_k)\n",
        "        attention_weights = torch.matmul(q, k.transpose(2, 1) )/ math.sqrt(self.d_k)\n",
        "\n",
        "\n",
        "        # ここでmaskを計算\n",
        "        #mask = mask.unsqueeze(1)\n",
        "        #attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # softmaxで規格化をする\n",
        "        normlized_weights = F.softmax(attention_weights, dim=-1)\n",
        "\n",
        "        # AttentionをValueとかけ算\n",
        "        output = torch.matmul(normlized_weights, v)\n",
        "\n",
        "        # 全結合層で特徴量を変換\n",
        "        output = self.out(output)\n",
        "        \n",
        "        return output, normlized_weights\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, classes, num_heads):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    self.head_dim = d_model // num_heads\n",
        "    self.seq_length = classes\n",
        "   \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = nn.Linear(d_model, d_model)\n",
        "    self.wk = nn.Linear(d_model, d_model)\n",
        "    self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.d_k = d_model\n",
        "\n",
        "    # 出力時に使用する全結合層\n",
        "    self.out = nn.Linear(d_model, d_model)\n",
        "        # 出力時に使用する全結合層\n",
        "    self.out = nn.Linear(d_model, d_model)\n",
        "    self._reset_parameters()\n",
        "        \n",
        " ## def split_heads(self, x, batch_size):\n",
        "    \"\"\"最後の次元を(num_heads, depth)に分割。\n",
        "    結果をshapeが(batch_size, num_heads, seq_len, depth)となるようにリシェイプする。\n",
        "    \"\"\"\n",
        "  ##  x = torch.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        " ##   return torch.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def _reset_parameters(self):\n",
        "    # Original Transformer initialization, see PyTorch documentation\n",
        "    nn.init.xavier_uniform_(self.wq.weight)\n",
        "    self.wq.bias.data.fill_(0)\n",
        "\n",
        "    nn.init.xavier_uniform_(self.wk.weight)\n",
        "    self.wk.bias.data.fill_(0)\n",
        "\n",
        "    nn.init.xavier_uniform_(self.wv.weight)\n",
        "    self.wv.bias.data.fill_(0)\n",
        "\n",
        "    \n",
        "  def forward(self, pe):\n",
        "    \n",
        "    #batch_sizeを指定\n",
        "    batch_size = 4\n",
        "    \n",
        "    q = self.wq(pe)\n",
        "    k = self.wk(pe)\n",
        "    v = self.wv(pe)\n",
        "    d_k = self.d_k\n",
        "\n",
        "\n",
        "    q = q.reshape(batch_size, self.seq_length, self.num_heads, self.head_dim)\n",
        "    k = k.reshape(batch_size, self.seq_length, self.num_heads, self.head_dim)\n",
        "    v = v.reshape(batch_size, self.seq_length, self.num_heads, self.head_dim)\n",
        "\n",
        "\n",
        "    #q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    #k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    #v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    \n",
        "    #attention_weights = torch.matmul(q, k.transpose(-2, -1))\n",
        "    #attention_weights = attention_weights / math.sqrt(d_k)\n",
        "\n",
        "    #attention_weights = torch.matmul(q.unsqueeze(1), k.transpose(-2, -1) )/ math.sqrt(self.d_k)\n",
        "    attention_weights = torch.matmul(q, k.transpose(3, 2) )/ math.sqrt(self.d_k)  \n",
        "    \n",
        "    # ここでmaskを計算\n",
        "    #mask = mask.unsqueeze(1)\n",
        "    #attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n",
        "    # softmaxで規格化をする\n",
        "    \n",
        "    normlized_weights = F.softmax(attention_weights, dim=-1)\n",
        "    # AttentionをValueとかけ算\n",
        "    values = torch.matmul(normlized_weights, v)\n",
        "    # 全結合層で特徴量を変換\n",
        "\n",
        "    values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "    values = values.reshape(batch_size,self.seq_length, self.d_model)\n",
        "    output = self.out(values)\n",
        "    \n",
        "    return output, normlized_weights\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model = 768, d_ff=1536, dropout=0.1):\n",
        "        '''Attention層から出力を単純に全結合層2つで特徴量を変換するだけのユニットです'''\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_1(x)\n",
        "        x = self.dropout(F.relu(x))\n",
        "        x = self.linear_2(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model=768, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # LayerNormalization層\n",
        "        self.norm_1 = nn.LayerNorm(d_model)\n",
        "        self.norm_2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Attention層\n",
        "        #self.attn = Attention(d_model=d_model)\n",
        "        self.attn = MultiHeadAttention(d_model=768, classes=14,  num_heads=4)\n",
        "\n",
        "        # Attentionのあとの全結合層2つ\n",
        "        self.ff = FeedForward(d_model)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,  x):\n",
        "        # PositionalEncoding を行った後に正規化する\n",
        "        x_normlized = self.norm_1(x)\n",
        "\n",
        "        output, normlized_weights = self.attn(x_normlized)\n",
        "        #FeedForward層の入力作成\n",
        "        x2 = x + self.dropout_1(output)\n",
        "\n",
        "        # 正規化と全結合層\n",
        "        x_normlized2 = self.norm_2(x2)\n",
        "        output = x2 + self.dropout_2(self.ff(x_normlized2))\n",
        "\n",
        "\n",
        "        return output, normlized_weights\n",
        "\n",
        "#MLP層\n",
        "class ClassificationHead(nn.Module):\n",
        "    '''Transformer_Blockの出力を使用し、最後にクラス分類させる'''\n",
        "\n",
        "    def __init__(self, d_model=768, d_model2=384, output_dim=14):\n",
        "        super().__init__()\n",
        "\n",
        "        # 全結合層\n",
        "        self.linear1 = nn.Linear(d_model, d_model2)  # output_dimはポジ・ネガの2つ\n",
        "        self.linear2 = nn.Linear(d_model2, output_dim)  # output_dimはポジ・ネガの2つ\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.linear1.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear1.bias, 0)\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.linear2.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear2.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "      #バッチサイズに合わせて変化\n",
        "        #x0 = torch.stack([torch.mean(x[0,:,:],0), torch.mean(x[1,:,:], 0), torch.mean(x[2,:,:],0), torch.mean(x[3,:,:], 0)], 0)\n",
        "        x0 = x[:, 0, :]  # 各ミニバッチの各文の cls の特徴量（768次元）を取り出す\n",
        "        x1 = F.relu(self.linear1(x0))\n",
        "        x2 = self.linear2(x1)\n",
        "\n",
        "        return x0, x2\n",
        "        \n",
        "# 最終的なTransformerモデルのクラス\n",
        "\n",
        "class TransformerClassification(nn.Module):\n",
        "    '''Transformerでクラス分類させる'''\n",
        "\n",
        "    def __init__(self,  d_model=768, max_seq_len=14, d_model2=384, output_dim=14):\n",
        "        super().__init__()\n",
        "\n",
        "        # モデル構築\n",
        "        \n",
        "        #self.net1 = BERT_net\n",
        "        self.net2 = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n",
        "        self.net3_1 = TransformerBlock(d_model=d_model)\n",
        "        self.net3_2 = TransformerBlock(d_model=d_model)\n",
        "        self.net4 = ClassificationHead(output_dim=output_dim,d_model2=d_model2,  d_model=d_model)\n",
        "\n",
        "    def forward(self, bert_cls1, x1, bert_cls2, x2, bert_cls3, x3, bert_cls4, x4, bert_cls5, x5, bert_cls6, x6, bert_cls7, x7):\n",
        "\n",
        "        q = torch.stack([ bert_cls1, x1, bert_cls2, x2, bert_cls3, x3, bert_cls4, x4, bert_cls5, x5, bert_cls6, x6, bert_cls7, x7], dim=1)\n",
        "        \n",
        "        q2 = self.net2(q)\n",
        "\n",
        "        x13, normlized_weights_1 = self.net3_1(q2)  # Self-Attentionで特徴量を変換\n",
        "        #x3_1_2 = x13[:, 0, :]\n",
        "\n",
        "        x3_2, normlized_weights_2 = self.net3_2(x13)  # Self-Attentionで特徴量を変換        \n",
        "       \n",
        "        tr_cls1, tr_cls2 = self.net4(x3_2)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力        [Transformer層での予測]\n",
        "\n",
        "\n",
        "        OO = nn.Sigmoid()\n",
        "        return OO(tr_cls2)\n",
        "      \n",
        "        #return OO(total_cls)  #normlized_weights_1, #normlized_weights_2\n"
      ],
      "metadata": {
        "id": "mGC4TboNOXy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "読み込んだテキストをトークナイズして, バッチサイズ 4 に分割します. BERT モデルを 14 個読み込んで transformer を接続するアンサンブル学習モデルの場合, バッチサイズ 4 を超えるとメモリがクラッシュしてしまうと思います. 単一の BERT モデルであればバッチサイズは何でもいいと思います."
      ],
      "metadata": {
        "id": "Kli5LCbUO4wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 前から2割をテストデータとする\n",
        "tr_df[:len_0_2].to_csv(\"./test.tsv\", sep='\\t', index=False, header=None)\n",
        "print(tr_df[:len_0_2].shape)\n",
        "\n",
        "# 前2割からを訓練&検証データとする\n",
        "tr_df[len_0_2:].to_csv(\"./train.tsv\", sep='\\t', index=False, header=None)\n",
        "print(tr_df[len_0_2:].shape)\n",
        "\n",
        "def tokenizer_512(input_text):\n",
        "    \"\"\"torchtextのtokenizerとして扱えるように、512単語のpytorchでのencodeを定義。ここで[0]を指定し忘れないように\"\"\"\n",
        "    return tokenizer.encode(input_text, max_length=100, return_tensors='pt')[0]\n",
        "\n",
        "\n",
        "TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer_512, use_vocab=False, lower=False,include_lengths=True, batch_first=True, fix_length=100, pad_token=0)\n",
        "# 注意：tokenize=tokenizer.encodeと、.encodeをつけます。padding[PAD]のindexが0なので、0を指定します。\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL1 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL2 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL3 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL4 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL5 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL6 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL7 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL8 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL9 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL10 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL11 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL12 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL13 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL14 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "# 乱数シードの固定\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED_VALUE = 1938  # これはなんでも良い\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)\n",
        "torch.manual_seed(SEED_VALUE)  # PyTorchを使う場合\n",
        "\n",
        "# torchtext.data.Datasetのsplit関数で訓練データと検証データを分ける\n",
        "# train_eval：4000個、test：2000個\n",
        "\n",
        "#dataset_train, dataset_eval = dataset_train_eval.split(split_ratio = 1 - 0.2, random_state=random.seed(1234))\n",
        "\n",
        "# 各tsvファイルを読み込み、分かち書きをしてdatasetに\n",
        "# train_eval：61300個、test：15324個\n",
        "dataset_train_eval, dataset_test = torchtext.legacy.data.TabularDataset.splits(\n",
        "    path='.', train='train.tsv', test='test.tsv', format='tsv', fields=[('Text', TEXT), ('Label1', LABEL1), ('Label2', LABEL2), ('Label3', LABEL3),  ('Label4', LABEL4), ('Label5', LABEL5), ('Label6', LABEL6), ('Label7', LABEL7), ('Label8', LABEL8), ('Label9', LABEL9), ('Label10', LABEL10), ('Label11', LABEL11), ('Label12', LABEL12), ('Label13', LABEL13), ('Label14', LABEL14)])\n",
        "# torchtext.data.Datasetのsplit関数で訓練データと検証データを分ける\n",
        "# train_eval：4000個、test：2000個\n",
        "\n",
        "dataset_train, dataset_eval = dataset_train_eval.split(split_ratio = 0.3, random_state=random.seed(1234))\n",
        "\n",
        "# datasetの長さを確認してみる\n",
        "print(\"train\", dataset_train.__len__())\n",
        "print(\"eval\", dataset_eval.__len__())\n",
        "print(\"test\", dataset_test.__len__())\n",
        "\n",
        "# datasetの中身を確認してみる\n",
        "item = next(iter(dataset_train))\n",
        "print(item.Text)\n",
        "print(\"長さ：\", len(item.Text))  # 長さを確認 [CLS]から始まり[SEP]で終わる。512より長いと後ろが切れる\n",
        "print(\"ラベル：\", item.Label1)\n",
        "\n",
        "# DataLoaderを作成\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "dl_train = torchtext.legacy.data.Iterator(\n",
        "    dataset_train_eval, batch_size=batch_size, train=True, shuffle=False)\n",
        "\n",
        "dl_eval = torchtext.legacy.data.Iterator(\n",
        "    dataset_eval, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "dl_test = torchtext.legacy.data.Iterator(\n",
        "    dataset_test, batch_size=batch_size, train=False, sort=False, shuffle=False)\n",
        "\n",
        "print(len(dl_train))\n",
        "print(len(dl_eval))\n",
        "print(len(dl_test))\n",
        "\n",
        "# 辞書オブジェクトにまとめる\n",
        "dataloaders_dict = {\"train\": dl_train, \"val\": dl_test}"
      ],
      "metadata": {
        "id": "Os7DE9G6O3V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT の訓練用の関数"
      ],
      "metadata": {
        "id": "rS1dh4kNQdfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルを学習させる関数を作成\n",
        "# GPUが使えるかを確認\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n",
        "print(\"使用デバイス：\", device)\n",
        "print('-----start-------')\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "    \n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    train_loss1 = []\n",
        "    val_loss1 = []\n",
        "    F1_list = []\n",
        "    F1_list2 = []\n",
        "    \n",
        "    precision_list1 = []\n",
        "    precision_list2 = []\n",
        "    \n",
        "    recall_list1 = []\n",
        "    recall_list2 = []\n",
        "\n",
        "    acc_train_list1 = []\n",
        "    acc_val_list1 = []\n",
        "\n",
        "    acc_train_list1 = []\n",
        "    acc_val_list1 = []\n",
        "\n",
        "    train_loss2 = []\n",
        "    val_loss2 = []\n",
        "\n",
        "    acc_train_list2 = []\n",
        "    acc_val_list2 = []\n",
        "\n",
        "    acc_train_list2 = []\n",
        "    acc_val_list2 = []\n",
        "\n",
        "    total_label_list = []\n",
        "    out_list = []\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # ミニバッチのサイズ\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        #for phase in ['train', 'val']:\n",
        "            \n",
        "            phase=\"train\"            \n",
        "\n",
        "            pred_list = []\n",
        "            label_list = []\n",
        "\n",
        "            net.train()  # モデルを訓練モードに\n",
        "            \n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "            iteration = 1\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[\"train\"]):\n",
        "                # batchはTextとLableの辞書型変数\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                inputs = batch.Text[0].to(device)  # 文章\n",
        "                labels = batch.Label1.to(device)  # ラベル\n",
        "                labels2 = batch.Label2.to(device)\n",
        "                labels3 = batch.Label3.to(device)\n",
        "                labels4 = batch.Label4.to(device)\n",
        "                labels5 = batch.Label5.to(device)\n",
        "                labels6 = batch.Label6.to(device)\n",
        "                labels7 = batch.Label7.to(device)\n",
        "                labels8 = batch.Label8.to(device)\n",
        "                labels9 = batch.Label9.to(device)\n",
        "                labels10 = batch.Label10.to(device)\n",
        "                labels11 = batch.Label11.to(device)\n",
        "                labels12 = batch.Label12.to(device)\n",
        "                labels13 = batch.Label13.to(device)\n",
        "                labels14 = batch.Label14.to(device)\n",
        "                pa = torch.stack([labels, labels2, labels3, labels4, labels5,labels6,labels7,labels8,labels9,labels10, labels11, labels12,labels13,labels14], dim = 1)\n",
        "                pa = torch.tensor(pa,  dtype=torch.float32)\n",
        "                \n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                #with torch.set_grad_enabled(phase == 'train'):    \n",
        "\n",
        "                tr_cls = net(inputs)\n",
        "\n",
        "#Transformer層の出力での予測\n",
        "                loss = criterion(tr_cls, pa)\n",
        "\n",
        "\n",
        "                #LABEL_cls, BERT_pred = LabelCls_convert(outputs)\n",
        "                ang = torch.round(tr_cls)\n",
        "                aa = ang.cpu().detach().numpy()\n",
        "                bb = pa.cpu().detach().numpy()\n",
        "                pred_list = np.append(pred_list, aa)\n",
        "                label_list = np.append(label_list, bb)\n",
        "\n",
        "\n",
        "#Transformer層の出力での予測\n",
        "\n",
        "                #if phase == \"train\":\n",
        "                # 訓練時は逆誤差伝搬\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                        \n",
        "                # 損失と正解数の合計を更新\n",
        "                epoch_loss += loss.item() * batch_size\n",
        "\n",
        "\n",
        "            # epochごとのlossと正解率\n",
        "#BERT       \n",
        "            #f12 = sum(f1) / len(f1)\n",
        "            #F1_list = np.append(F1_list, f12)\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            precisionscore = precision_score(pred_list.reshape(int(len(pred_list)/14), 14), label_list.reshape(int(len(label_list)/14), 14), average= 'micro') \n",
        "            recallscore = recall_score(pred_list.reshape(int(len(pred_list)/14), 14), label_list.reshape(int(len(label_list)/14), 14), average= 'micro')\n",
        "            f1score = f1_score(pred_list.reshape(int(len(pred_list)/14), 14), label_list.reshape(int(len(label_list)/14), 14), average= 'micro')\n",
        "            \n",
        "\n",
        "#Transformer\n",
        "\n",
        "\n",
        "            #if phase == 'train':\n",
        "            train_loss1 = np.append(train_loss1, epoch_loss)\n",
        "            print(\"f1\",f1score)\n",
        "            print(\"precision\", precisionscore)\n",
        "            print(\"recaoll\", recallscore)\n",
        "            precision_list1 = np.append(precision_list1, precisionscore)\n",
        "            recall_list1 = np.append(recall_list1, recallscore)\n",
        "            F1_list2 = np.append(F1_list2, f1score)\n",
        "\n",
        "              #cm1 = confusion_matrix(label_list, pred_list)                       #Transformer\n",
        "                  #BERT\n",
        "              \n",
        "              #print(\"\\n BERT 混合行列\", cm1, sep=\"\\n\")\n",
        "\n",
        "            print('Epoch of BERT {}/{} | {:^5} | Loss: {:.4f} F1: {:.4f}'.format(epoch+1, num_epochs, phase, epoch_loss, f1score))\n",
        "            \n",
        "    \n",
        "    #out_list = out_list.reshape(int(len(out_list)/2), 2)\n",
        "    return net,  train_loss1, val_loss1, F1_list2, F1_list, precision_list1, precision_list2, recall_list1, recall_list2, pred_list"
      ],
      "metadata": {
        "id": "eFKzIAJBQcF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT の訓練用のコード"
      ],
      "metadata": {
        "id": "1TbV2LCFQzq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f12=0\n",
        "ec=0\n",
        "for i in range(1,11, 1):\n",
        "\n",
        "  # モデル構築\n",
        "  BERT_net = BertForReview()\n",
        "  # 訓練モードに設定\n",
        "  BERT_net.train()\n",
        "  # 1. まず全部を、勾配計算Falseにしてしまう\n",
        "  for param in BERT_net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
        "  for param in BERT_net.bert.encoder.layer[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 3. 識別器を勾配計算ありに変更\n",
        "  for param in BERT_net.cls.parameters():\n",
        "    param.requires_grad = True\n",
        "  \n",
        "  B = i/10000000\n",
        "\n",
        "  bert_optimizer = optim.Adam([\n",
        "    {'params': BERT_net.bert.encoder.layer[-1].parameters(), 'lr1': 6e-6},\n",
        "    {'params': BERT_net.cls.parameters(), 'lr2': B}\n",
        "  ])\n",
        "  print(\"lr\", B)\n",
        "\n",
        "  net_trained, trainloss, valloss, Ftrain, Fval, Pretrain1, Preval1, Rectrain1, Recval1,  PRED_LIST1 = train_model(BERT_net,  dataloaders_dict,criterion, bert_optimizer, num_epochs=20)\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  net_trained.eval()   # モデルを検証モードに\n",
        "  net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "# epochの正解数を記録する変数\n",
        "  epoch_corrects = 0\n",
        "  pred_list1 = []\n",
        "  label_list1 = []\n",
        "  aa1 = 0\n",
        "  bb1 = 0\n",
        "\n",
        "  acc_count=0\n",
        "  part_acc_count=0\n",
        "  all_miss=0\n",
        "\n",
        "\n",
        "  for batch in tqdm(dl_test): \n",
        "    inputs = batch.Text[0].to(device)  # 文章\n",
        "    labels0 = batch.Label1.to(device)  # ラベル\n",
        "    labels1 = batch.Label2.to(device)\n",
        "    labels2 = batch.Label3.to(device)\n",
        "    labels3 = batch.Label4.to(device)\n",
        "    labels4 = batch.Label5.to(device)\n",
        "    labels5 = batch.Label6.to(device)\n",
        "    labels6 = batch.Label7.to(device)\n",
        "    labels7 = batch.Label8.to(device)\n",
        "    labels8 = batch.Label9.to(device)\n",
        "    labels9 = batch.Label10.to(device)\n",
        "    labels10 = batch.Label11.to(device)\n",
        "    labels11 = batch.Label12.to(device)\n",
        "    labels12 = batch.Label13.to(device)\n",
        "    labels13 = batch.Label14.to(device)\n",
        "    pa = torch.stack([labels0, labels1, labels2, labels3, labels4,labels5,labels6,labels7,labels8,labels9, labels10, labels11,labels12,labels13], dim = 1)\n",
        "    pa2 = pa.cpu().numpy()\n",
        "      \n",
        "\n",
        "    Outputs = net_trained(inputs)\n",
        "\n",
        "    preds = torch.round(Outputs)\n",
        "\n",
        "    aa1 = preds.cpu().detach().numpy()\n",
        "    bb1 = pa.cpu().detach().numpy()\n",
        "    pred_list1 = np.append(pred_list1, aa1)\n",
        "    label_list1 = np.append(label_list1, bb1)\n",
        "\n",
        "  precisionscore = precision_score(pred_list1.reshape(int(len(pred_list1)/14), 14), label_list1.reshape(int(len(label_list1)/14), 14), average= 'micro') \n",
        "  recallscore = recall_score(pred_list1.reshape(int(len(pred_list1)/14), 14), label_list1.reshape(int(len(label_list1)/14), 14), average= 'micro')\n",
        "  epoch_f1 = f1_score(pred_list1.reshape(int(len(pred_list1)/14), 14), label_list1.reshape(int(len(label_list1)/14), 14), average= 'micro')\n",
        "        \n",
        "  print('テストデータ{}個での正解率：{:.4f}'.format(len(dl_test.dataset), precisionscore))\n",
        "        \n",
        "  print('テストデータ{}個での正解率：{:.4f}'.format(len(dl_test.dataset), recallscore))\n",
        "        \n",
        "  print('テストデータ{}個での正解率：{:.4f}'.format(len(dl_test.dataset), epoch_f1))\n",
        "\n",
        "  if epoch_f1>f12:\n",
        "    f12=epoch_f1\n",
        "    ec=ec+1\n",
        "\n",
        "  if ec>6:\n",
        "    print(\"########################################################################################################################################################\")\n",
        "\n",
        "  PRED_LIST = pred_list1.reshape(int(len(pred_list1)/14), 14)\n",
        "  LABEL_LIST = label_list1.reshape(int(len(label_list1)/14), 14)\n",
        "\n",
        "  for i in range(len(PRED_LIST)):\n",
        "    if (PRED_LIST[i]==LABEL_LIST[i]).all():\n",
        "      acc_count+=1\n",
        "    elif (PRED_LIST[i]==LABEL_LIST[i]).any():\n",
        "      part_acc_count+=1\n",
        "  all_miss = len(dl_test.dataset) - (acc_count + part_acc_count)\n",
        "  \n",
        "  print(\"######################################################\")\n",
        "  print(\"ACC_COUNT\", acc_count)\n",
        "  print(\"PART_ACC_COUNT\", part_acc_count)\n",
        "  print(\"ALL_MISS\", all_miss)"
      ],
      "metadata": {
        "id": "egz7dqJyQrKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "作成した事前学習済みの BERT ベースの 2 値分類モデルを net_trained11 などに格納して, それぞれのモデルから CLS トークンを抽出する関数"
      ],
      "metadata": {
        "id": "rHGHT_KeRfU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # GPUが使えるかを確認\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"使用デバイス：\", device)\n",
        "print('-----start-------')\n",
        "\n",
        "def train_model(net_trained11, net_trained12, net_trained21, net_trained22, net_trained31, net_trained32, net_trained41, net_trained42, net_trained51, net_trained52, net_trained61, net_trained62, net_trained71, net_trained72, dataloaders_dict, criterion, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "    # ネットワークをGPUへ\n",
        "    \n",
        "    net_trained11.eval()   # モデルを検証モードに\n",
        "    net_trained11.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "    net_trained12.eval()   # モデルを検証モードに\n",
        "    net_trained12.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "    \n",
        "    net_trained21.eval()   # モデルを検証モードに\n",
        "    net_trained21.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "    net_trained22.eval()   # モデルを検証モードに\n",
        "    net_trained22.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "    \n",
        "    net_trained31.eval()   # モデルを検証モードに\n",
        "    net_trained31.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "    net_trained32.eval()   # モデルを検証モードに\n",
        "    net_trained32.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "    \n",
        "    net_trained41.eval()   # モデルを検証モードに\n",
        "    net_trained41.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "    net_trained42.eval()   # モデルを検証モードに\n",
        "    net_trained42.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "    \n",
        "    net_trained51.eval()   # モデルを検証モードに\n",
        "    net_trained51.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "    net_trained52.eval()   # モデルを検証モードに\n",
        "    net_trained52.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "    \n",
        "    net_trained61.eval()   # モデルを検証モードに\n",
        "    net_trained61.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "    net_trained62.eval()   # モデルを検証モードに\n",
        "    net_trained62.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "    \n",
        "    net_trained71.eval()   # モデルを検証モードに\n",
        "    net_trained71.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "    net_trained72.eval()   # モデルを検証モードに\n",
        "    net_trained72.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "\n",
        "    train_loss1 = []\n",
        "    val_loss1 = []\n",
        "    F1_list = []\n",
        "    F1_list2 = []\n",
        "\n",
        "    acc_train_list1 = []\n",
        "    acc_val_list1 = []\n",
        "\n",
        "    acc_train_list1 = []\n",
        "    acc_val_list1 = []\n",
        "\n",
        "    train_loss2 = []\n",
        "    val_loss2 = []\n",
        "\n",
        "    acc_train_list2 = []\n",
        "    acc_val_list2 = []\n",
        "\n",
        "    acc_train_list2 = []\n",
        "    acc_val_list2 = []\n",
        "\n",
        "    total_label_list = []\n",
        "    out_list = []\n",
        "\n",
        "    INP=[]\n",
        "\n",
        "\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        ext=[]\n",
        "        b1=[]\n",
        "        b2=[]\n",
        "        b3=[]\n",
        "        b4=[]\n",
        "        b5=[]\n",
        "        b6=[]\n",
        "        b7=[]\n",
        "\n",
        "        c1=[]\n",
        "        c2=[]\n",
        "        c3=[]\n",
        "        c4=[]\n",
        "        c5=[]\n",
        "        c6=[]\n",
        "        c7=[]\n",
        "\n",
        "        p = []\n",
        "        ins=[]\n",
        "\n",
        "\n",
        "\n",
        "        f1 = 0\n",
        "        f12 = 0\n",
        "        f1_list = []\n",
        "        f1_list2 = []\n",
        "\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            \n",
        "            pred_list = []\n",
        "            label_list = []\n",
        "\n",
        "            pred_list2 = []\n",
        "            label_list2 = []\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_loss1 = 0.0  # epochの損失和\n",
        "            epoch_corrects1 = 0  # epochの正解数\n",
        "            epoch_loss2 = 0.0  # epochの損失和\n",
        "            epoch_corrects2 = 0  # epochの正解数\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書オブジェクト\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                inputs = batch.Text[0].to(device)  # 文章\n",
        "                labels0 = batch.Label1.to(device)  # ラベル\n",
        "                labels1 = batch.Label2.to(device)\n",
        "                labels2 = batch.Label3.to(device)\n",
        "                labels3 = batch.Label4.to(device)\n",
        "                labels4 = batch.Label5.to(device)\n",
        "                labels5 = batch.Label6.to(device)\n",
        "                labels6 = batch.Label7.to(device)\n",
        "                labels7 = batch.Label8.to(device)\n",
        "                labels8 = batch.Label9.to(device)\n",
        "                labels9 = batch.Label10.to(device)\n",
        "                labels10 = batch.Label11.to(device)\n",
        "                labels11 = batch.Label12.to(device)\n",
        "                labels12 = batch.Label13.to(device)\n",
        "                labels13 = batch.Label14.to(device)\n",
        "                pa = torch.stack([labels0, labels1, labels2, labels3, labels4,labels5,labels6,labels7,labels8,labels9, labels10, labels11,labels12,labels13], dim = 1)\n",
        "                \n",
        "                pa = torch.tensor(pa, dtype=torch.float32)\n",
        "                \n",
        "                #pa2 = torch.tensor(inputs, dtype=torch.float32)\n",
        "             # optimizerを初期化\n",
        "                #optimizer1.zero_grad()\n",
        "                #optimizer2.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                #if phase == \"train\":\n",
        "                    \n",
        "                Outputs11, BERT_atten11, BERT_cls11 = net_trained11(inputs)\n",
        "                Outputs12, BERT_atten12, BERT_cls12 = net_trained12(inputs)\n",
        "\n",
        "                Outputs21, BERT_atten21, BERT_cls21 = net_trained21(inputs)\n",
        "                Outputs22, BERT_atten22, BERT_cls22 = net_trained22(inputs)\n",
        "      \n",
        "                Outputs31, BERT_atten31, BERT_cls31 = net_trained31(inputs)\n",
        "                Outputs32, BERT_atten32, BERT_cls32 = net_trained32(inputs)\n",
        "                \n",
        "                Outputs41, BERT_atten41, BERT_cls41 = net_trained41(inputs)\n",
        "                Outputs42, BERT_atten42, BERT_cls42 = net_trained42(inputs)\n",
        "    \n",
        "                Outputs51, BERT_atten51, BERT_cls51 = net_trained51(inputs)\n",
        "                Outputs52, BERT_atten52, BERT_cls52 = net_trained52(inputs)\n",
        "              \n",
        "                Outputs61, BERT_atten61, BERT_cls61 = net_trained61(inputs)\n",
        "                Outputs62, BERT_atten62, BERT_cls62 = net_trained62(inputs)\n",
        "                \n",
        "                Outputs71, BERT_atten71, BERT_cls71 = net_trained71(inputs)\n",
        "                Outputs72, BERT_atten72, BERT_cls72 = net_trained72(inputs)\n",
        "\n",
        "                b1.append(BERT_cls11.cpu().detach().numpy())\n",
        "                b2.append(BERT_cls21.cpu().detach().numpy())\n",
        "                b3.append(BERT_cls31.cpu().detach().numpy())\n",
        "                b4.append(BERT_cls41.cpu().detach().numpy())\n",
        "                b5.append(BERT_cls51.cpu().detach().numpy())\n",
        "                b6.append(BERT_cls61.cpu().detach().numpy())\n",
        "                b7.append(BERT_cls71.cpu().detach().numpy())\n",
        "                \n",
        "                c1.append(BERT_cls12.cpu().detach().numpy())\n",
        "                c2.append(BERT_cls22.cpu().detach().numpy())\n",
        "                c3.append(BERT_cls32.cpu().detach().numpy())\n",
        "                c4.append(BERT_cls42.cpu().detach().numpy())\n",
        "                c5.append(BERT_cls52.cpu().detach().numpy())\n",
        "                c6.append(BERT_cls62.cpu().detach().numpy())\n",
        "                c7.append(BERT_cls72.cpu().detach().numpy())\n",
        "                p.append(pa)\n",
        "                ins.append(inputs)\n",
        "\n",
        "\n",
        "                #if phase == \"val\":\n",
        "\n",
        "\n",
        "\n",
        "    return b1, b2, b3, b4,b5,b6,b7,c1,c2,c3,c4,c5,c6,c7, p, ins"
      ],
      "metadata": {
        "id": "HJE_bSurRKOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "エポック 1 で CLS トークンをすべて抽出. 30000 データで 1 時間弱かかります."
      ],
      "metadata": {
        "id": "YB8XgYv8TPSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "b1,b2,b3,b4,b5,b6,b7,c1,c2,c3,c4,c5,c6,c7, p, inp = train_model(bremo_ca, bremo_se, dinmo_ca, dinmo_se, batmo_ca, sermo_se, sermo_ca, stamo_se, stamo_ca, stamo_se, facmo_ca, facmo_se, roomo_ca, roomo_se, dataloaders_dict, criterion, num_epochs=num_epochs)"
      ],
      "metadata": {
        "id": "x3ObKLWATL8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "抽出した CLS トークンを test1 などに格納して Transformer で学習する関数"
      ],
      "metadata": {
        "id": "dnlVqplCUqse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tr_test(net_trained):\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "  net_trained.eval()   # モデルを検証モードに\n",
        "  net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "# epochの正解数を記録する変数\n",
        "  epoch_corrects = 0\n",
        "  pred_list1 = []\n",
        "  label_list1 = []\n",
        "  aa1 = 0\n",
        "  bb1 = 0\n",
        "\n",
        "\n",
        "  acc_count=0\n",
        "  part_acc_count=0\n",
        "  all_miss=0\n",
        "\n",
        "  # データローダーからミニバッチを取り出すループ\n",
        "  for (b12, b22, b32, b42,b52,b62,b72,c12,c22,c32,c42,c52,c62,c72, p2) in zip(test1, test2, test3, test4, test5, test6, test7, test8, test9, test10, test11, test12, test13, test14, test_target):\n",
        "  #zip(b1[vallen],b2[vallen],b3[vallen],b4[vallen],b5[vallen],b6[vallen],b7[vallen],c1[vallen],c2[vallen],c3[vallen],c4[vallen],c5[vallen],c6[vallen],c7[vallen], p[vallen]):\n",
        "    labels0 = torch.tensor(b12).to(device)  # ラベル\n",
        "    labels1 = torch.tensor(b22).to(device)\n",
        "    labels2 = torch.tensor(b32).to(device)\n",
        "    labels3 = torch.tensor(b42).to(device)\n",
        "    labels4 = torch.tensor(b52).to(device)\n",
        "    labels5 = torch.tensor(b62).to(device)\n",
        "    labels6 = torch.tensor(b72).to(device)\n",
        "    labels7 = torch.tensor(c12).to(device)\n",
        "    labels8 = torch.tensor(c22).to(device)\n",
        "    labels9 = torch.tensor(c32).to(device)\n",
        "    labels10 = torch.tensor(c42).to(device)\n",
        "    labels11 = torch.tensor(c52).to(device)\n",
        "    labels12 = torch.tensor(c62).to(device)\n",
        "    labels13 = torch.tensor(c72).to(device)\n",
        "    pa = torch.tensor(p2, dtype=torch.float32).to(device)\n",
        "                \n",
        "    pa2 = pa.cpu().numpy()\n",
        "    # optimizerを初期化\n",
        "    #label_class1 = pa2[:, :2]\n",
        "    #0クラス目のデータの諸々を取得し、学習#######################################################\n",
        "    #pa31 = Pa_Numpy(label_class1)\n",
        "    #正解ラベル\n",
        "    #pa4_0 = torch.tensor(pa31, dtype=torch.int64).to(device)\n",
        "  \n",
        "    tr_cls = net_trained(labels7, labels0, labels8, labels1, labels9, labels2, labels10, labels3, labels11, labels4, labels12, labels5, labels13, labels6)\n",
        "  #loss1 = criterion(tr_cls, )\n",
        "\n",
        "    preds = torch.round(tr_cls)  # ラベルを予測\n",
        "\n",
        "    aa = preds.cpu().detach().numpy()\n",
        "    bb = pa.cpu().detach().numpy()\n",
        "    pred_list1 = np.append(pred_list1, aa)\n",
        "    label_list1 = np.append(label_list1, bb)\n",
        "\n",
        "  B=label_list1.reshape(int(len(label_list1)/14), 14)\n",
        "  A=pred_list1.reshape(int(len(pred_list1)/14), 14)\n",
        "\n",
        "  PRED_LIST = A\n",
        "  LABEL_LIST = B\n",
        "\n",
        "  for i in range(len(PRED_LIST)):\n",
        "    if (PRED_LIST[i]==LABEL_LIST[i]).all():\n",
        "      acc_count+=1\n",
        "    elif (PRED_LIST[i]==LABEL_LIST[i]).any():\n",
        "      part_acc_count+=1\n",
        "  all_miss = int(len(test1)*4) - (acc_count + part_acc_count)\n",
        "  \n",
        "  print(\"######################################################\")\n",
        "  print(\"ACC_COUNT\", acc_count)\n",
        "  print(\"PART_ACC_COUNT\", part_acc_count)\n",
        "  print(\"ALL_MISS\", all_miss)\n",
        "  \n",
        "  return A, B\n",
        "\n",
        "##################################################################################################################\n",
        "A,B = tr_test(tr_encoder)\n",
        "\n",
        "precisionscore = precision_score(A,B, average=\"micro\")\n",
        "recallscore = recall_score(A,B, average=\"micro\")\n",
        "f1score = f1_score(A,B, average=\"micro\")\n",
        "print(\"テストデータ :\", int(len(test1)*4))\n",
        "print(\"VAL_Precision\", precisionscore)\n",
        "print(\"VAL_Recall\", recallscore)\n",
        "print(\"VAL_F1\", f1score)\n",
        "\n",
        "all_c=[]\n",
        "part_c=[]\n",
        "\n",
        "PRED_LIST = A\n",
        "LABEL_LIST = B\n",
        "acc_count=0\n",
        "part_acc_count=0\n",
        "all_miss=0\n",
        "\n",
        "for i in range(len(PRED_LIST)):\n",
        "    if (PRED_LIST[i]==LABEL_LIST[i]).all():\n",
        "      all_c.append(i)\n",
        "      acc_count+=1\n",
        "    elif (PRED_LIST[i]==LABEL_LIST[i]).any():\n",
        "      part_c.append(i)\n",
        "      part_acc_count+=1\n",
        "all_miss = int(len(test1)*4) - (acc_count + part_acc_count)\n",
        "  \n",
        "print(\"######################################################\")\n",
        "print(\"ACC_COUNT\", acc_count)\n",
        "print(\"PART_ACC_COUNT\", part_acc_count)\n",
        "print(\"ALL_MISS\", all_miss)"
      ],
      "metadata": {
        "id": "T1aCTJDzUpka"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}