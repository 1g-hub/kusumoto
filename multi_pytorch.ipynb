{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXZINHKrrA9QEENB4DTEBR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuuki-kusumoto/kusumoto/blob/master/multi_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R33fK3YUA5jV"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opCrjrJA37N"
      },
      "source": [
        "!apt install aptitude swig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a8ii-gFA_Tg"
      },
      "source": [
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duc4HlAwBCo8"
      },
      "source": [
        "pip install MeCab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eYt8DylBC44"
      },
      "source": [
        "pip install mecab-python3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj6YA-6xBDF4"
      },
      "source": [
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlHAzbV2BDTp"
      },
      "source": [
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbwfqvtFBDkC"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n",
        "path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
        "                           shell=True).communicate()[0]).decode('utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX93qn7kBD_c"
      },
      "source": [
        "pip install neologdn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O-ochNHBEO8"
      },
      "source": [
        "pip install ipadic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlNZJ7MFBEdQ"
      },
      "source": [
        "pip install mecab-python3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUE1TK5tBZ0W"
      },
      "source": [
        "pip install unidic-lite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRYTwO6WBZw0",
        "outputId": "0fe40e8b-7a92-48e8-bacf-da40d4105b98"
      },
      "source": [
        "# MeCabとtransformersを用意する\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "# 以下で報告があるようにmecab-python3のバージョンを0.996.5にしないとtokezerで落ちる\n",
        "# https://stackoverflow.com/questions/62860717/huggingface-for-japanese-tokenizer\n",
        "!pip install mecab-python3==0.996.5\n",
        "!pip install unidic-lite # これないとMeCab実行時にエラーで落ちる\n",
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 6.8MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 37.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 42.1MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x75S-XvoBZuJ"
      },
      "source": [
        "pip install \"transformers==2.5.1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhQLbD9yKe0e"
      },
      "source": [
        "import torch\n",
        "from transformers.modeling_bert import BertModel\n",
        "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8tdeog2BZpE"
      },
      "source": [
        "! curl http://www.cl.ecei.tohoku.ac.jp/resources/sent_lex/pn.csv.m3.120408.trim > pn.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_nuEq9YBZmG"
      },
      "source": [
        "import transformers\n",
        "from transformers.modeling_bert import BertModel\n",
        "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
        "\n",
        "model = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
        "\n",
        "# model_nameはここから取得(cf. https://huggingface.co/transformers/pretrained_models.html)\n",
        "model_name = \"cl-tohoku/bert-base-japanese\"\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2k2dVA0D9A4"
      },
      "source": [
        "CSVデータの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JNqnMPlBZhQ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_csv('travel.csv', index_col=0)\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQO8wluoBZVU"
      },
      "source": [
        "df1.columns=['text','breakfast_po',\t'breakfast_ne',\t'dinner_po',\t'dinner_ne',\t'bath_po',\t'bath_ne',\t'servis_po',\t'service_ne',\t'state_po',\t'state_ne',\t'facility_po',\t'facility_ne'\t,'room_po', 'room_ne']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2zGoP35EJUP"
      },
      "source": [
        "df = df1.fillna(0)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7qVMJqtEJQj"
      },
      "source": [
        "print(df.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AorDIs5BEYwO"
      },
      "source": [
        "データフレーム内の値をfloat型からobject型に変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNWbgo6HEJOA"
      },
      "source": [
        "df_int2 = df[['breakfast_ne',\t'dinner_po',\t'dinner_ne',\t'bath_po',\t'bath_ne',\t'servis_po',\t'service_ne',\t'state_po',\t'state_ne',\t'facility_po',\t'facility_ne'\t,'room_po', 'room_ne']].astype(int)\n",
        "print(df_int2.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tLq1EShEJLX"
      },
      "source": [
        "df_int2 = df[['breakfast_ne',\t'dinner_po',\t'dinner_ne',\t'bath_po',\t'bath_ne',\t'servis_po',\t'service_ne',\t'state_po',\t'state_ne',\t'facility_po',\t'facility_ne'\t,'room_po', 'room_ne']].astype(int)\n",
        "print(df_int2.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj7VNtEuEJIx"
      },
      "source": [
        "df_object = df_int2.astype(object)\n",
        "print(df_object.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbCIAhNSEJGb"
      },
      "source": [
        "df_int1 = df[['text','breakfast_po']]\n",
        "print(df_int1.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGde3WCBEJDW"
      },
      "source": [
        "#データフレームを結合させる\n",
        "df_concat = pd.concat([df_int1, df_object], axis =1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HztKcmz3EJA5"
      },
      "source": [
        "df = df_concat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKv0SFKHEI9h"
      },
      "source": [
        "df2 = df[['text','dinner_po',\t'dinner_ne',\t'bath_po',\t'bath_ne',\t'servis_po',\t'service_ne',\t'state_po',\t'state_ne',\t'facility_po',\t'facility_ne'\t,'room_po', 'room_ne']]\n",
        "df2 = df2.sample(frac=1, random_state=127).reset_index(drop=True)\n",
        "df2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmqrCspgEI7O"
      },
      "source": [
        "# tsvファイルで保存する\n",
        "\n",
        "# 全体の2割の文章数\n",
        "len_0_2 = len(df2) // 5\n",
        "\n",
        "# 前から2割をテストデータとする\n",
        "df2[:len_0_2].to_csv(\"./test.tsv\", sep='\\t', index=False, header=None)\n",
        "print(df2[:len_0_2].shape)\n",
        "\n",
        "# 前2割からを訓練&検証データとする\n",
        "df2[len_0_2:].to_csv(\"./train.tsv\", sep='\\t', index=False, header=None)\n",
        "print(df2[len_0_2:].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YEGptSgEI3_"
      },
      "source": [
        "# 乱数シードの固定\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED_VALUE = 1234\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)\n",
        "torch.manual_seed(SEED_VALUE)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epaaRYk5EI1M"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "import torch\n",
        "import torchtext  # torchtextを使用"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9oCHuzJEIwM"
      },
      "source": [
        "def tokenizer_512(input_text):\n",
        "    \"\"\"torchtextのtokenizerとして扱えるように、512単語のpytorchでのencodeを定義。ここで[0]を指定し忘れないように\"\"\"\n",
        "    return tokenizer.encode(input_text, max_length=512, return_tensors='pt')[0]\n",
        "\n",
        "\n",
        "TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer_512, use_vocab=False, lower=False,include_lengths=True, batch_first=True, fix_length=512, pad_token=0)\n",
        "# 注意：tokenize=tokenizer.encodeと、.encodeをつけます。padding[PAD]のindexが0なので、0を指定します。\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL2 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL3 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL4 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL5 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL6 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL7 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL8 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL9 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL10 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL11 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#ラベルは数字なのでラベル用オブジェクトはsequential = False\n",
        "LABEL12 = torchtext.legacy.data.Field(sequential=False, use_vocab=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGs9IcfaEItL"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from transformers.modeling_bert import BertModel\n",
        "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
        "\n",
        "\n",
        "# 日本語BERTの分かち書き用tokenizerを宣言\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDunrI-SFnoq"
      },
      "source": [
        "import transformers\n",
        "from transformers.modeling_bert import BertModel\n",
        "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
        "\n",
        "model = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
        "\n",
        "# model_nameはここから取得(cf. https://huggingface.co/transformers/pretrained_models.html)\n",
        "model_name = \"cl-tohoku/bert-base-japanese\"\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo2iVT9kFnl6"
      },
      "source": [
        "# 各tsvファイルを読み込み、分かち書きをしてdatasetに\n",
        "# train_eval：61300個、test：15324個\n",
        "dataset_train_eval, dataset_test = torchtext.legacy.data.TabularDataset.splits(\n",
        "    path='.', train='train.tsv', test='test.tsv', format='tsv', fields=[('Text', TEXT), ('Label', LABEL), ('Label2', LABEL2), ('Label3', LABEL3),  ('Label4', LABEL4), ('Label5', LABEL5), ('Label6', LABEL6), ('Label7', LABEL7), ('Label8', LABEL8), ('Label9', LABEL9), ('Label10', LABEL10), ('Label11', LABEL11), ('Label12', LABEL12)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqL0JugsFnjg"
      },
      "source": [
        "# torchtext.data.Datasetのsplit関数で訓練データと検証データを分ける\n",
        "# train_eval：61300個、test：15324個\n",
        "\n",
        "dataset_train, dataset_eval = dataset_train_eval.split(\n",
        "    split_ratio = 1 - 15324 / 61300, random_state=random.seed(1234))\n",
        "\n",
        "# datasetの長さを確認してみる\n",
        "print(dataset_train.__len__())\n",
        "print(dataset_eval.__len__())\n",
        "print(dataset_test.__len__())\n",
        "\n",
        "# datasetの中身を確認してみる\n",
        "item = next(iter(dataset_train))\n",
        "print(item.Text)\n",
        "print(\"長さ：\", len(item.Text))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_aQ0DvEFnfr"
      },
      "source": [
        "print('訓練データの数', len(dataset_train_eval))\n",
        "print('1つ目の訓練データkazu', vars(dataset_train_eval[10]))\n",
        "print(vars(dataset_train_eval[0])['Label'])\n",
        "print(len(vars(dataset_train_eval[0])))\n",
        "\n",
        "# datasetの中身を文章に戻し、確認\n",
        "\n",
        "print(tokenizer.convert_ids_to_tokens(item.Text.tolist()))  # 文章"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n96B5dVMFndX"
      },
      "source": [
        "#@title\n",
        "# 日本語BERTで扱える文章の長さは512\n",
        "import seaborn as sns\n",
        "title_length = df_concat2['text'].map(tokenizer.encode).map(len)\n",
        "print(max(title_length))\n",
        "\n",
        "sns.distplot(title_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qyli0ZrFnba"
      },
      "source": [
        "# DataLoaderの作成\n",
        "\n",
        "batch_size = 12\n",
        "\n",
        "\n",
        "dl_train = torchtext.legacy.data.Iterator(\n",
        "    dataset_train, batch_size=batch_size, train=True)\n",
        "\n",
        "dl_eval = torchtext.legacy.data.Iterator(\n",
        "    dataset_eval, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "dl_test = torchtext.legacy.data.Iterator(\n",
        "    dataset_test, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "# 辞書オブジェクトにまとめる\n",
        "dataloaders_dict = {\"train\": dl_train, \"val\": dl_eval}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOOKe5F0FnZG"
      },
      "source": [
        "# DataLoaderの動作確認 \n",
        "\n",
        "batch = next(iter(dl_test))\n",
        "print(batch)\n",
        "print(batch.Text[0].shape)\n",
        "print(batch.Label2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2KzUrNvFnWY"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class BertForReview(nn.Module):\n",
        "    '''BERTモデルにレビュー文の2クラスを判定する部分をつなげたモデル'''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BertForReview, self).__init__()\n",
        "\n",
        "        # BERTモジュール\n",
        "        self.bert = model  # 日本語学習済みのBERTモデル\n",
        "\n",
        "        # headにポジネガ予測を追加\n",
        "        # 入力はBERTの出力特徴量の次元768、出力は12クラス\n",
        "        self.cls = nn.Linear(in_features=768, out_features=12)\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.cls.weight, std=0.02)\n",
        "        nn.init.normal_(self.cls.bias, 0)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        '''\n",
        "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
        "        '''\n",
        "\n",
        "        # BERTの基本モデル部分の順伝搬\n",
        "        # 順伝搬させる\n",
        "        result = self.bert(input_ids)  # reult は、sequence_output, pooled_output\n",
        "\n",
        "        # sequence_outputの先頭の単語ベクトルを抜き出す\n",
        "        vec_0 = result[0]  # 最初の0がsequence_outputを示す\n",
        "        vec_0 = vec_0[:, 0, :]  # 全バッチ。先頭0番目の単語の全768要素\n",
        "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_size]に変換\n",
        "        output = self.cls(vec_0)  # 全結合層\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiJBHxHJFnN2"
      },
      "source": [
        "#モデル構築\n",
        "net = BertForReview()\n",
        "\n",
        "# 訓練モードに設定\n",
        "net.train()\n",
        "\n",
        "print('ネットワーク設定完了')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpypH1cxG7P6"
      },
      "source": [
        "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
        "\n",
        "# 1. まず全部を、勾配計算Falseにしてしまう\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
        "for param in net.bert.encoder.layer[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 3. 識別器を勾配計算ありに変更\n",
        "for param in net.cls.parameters():\n",
        "    param.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df6eeIryHCP4"
      },
      "source": [
        "# 最適化手法の設定\n",
        "import torch.optim as optim\n",
        "\n",
        "# BERTの元の部分はファインチューニング\n",
        "optimizer = optim.Adam([\n",
        "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 6e-6},\n",
        "    {'params': net.cls.parameters(), 'lr': 1e-4}\n",
        "])\n",
        "\n",
        "# 損失関数\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgejf987HCM3"
      },
      "source": [
        "# モデルを学習させる関数を作成\n",
        "\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # ミニバッチのサイズ\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "            iteration = 1\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書型変数\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                inputs = batch.Text[0].to(device)  # 文章\n",
        "                labels = batch.Label.to(device)  # ラベル\n",
        "                labels2 = batch.Label2.to(device)\n",
        "                labels3 = batch.Label3.to(device)\n",
        "                labels4 = batch.Label4.to(device)\n",
        "                labels5 = batch.Label5.to(device)\n",
        "                labels6 = batch.Label6.to(device)\n",
        "                labels7 = batch.Label7.to(device)\n",
        "                labels8 = batch.Label8.to(device)\n",
        "                labels9 = batch.Label9.to(device)\n",
        "                labels10 = batch.Label10.to(device)\n",
        "                labels11 = batch.Label11.to(device)\n",
        "                labels12 = batch.Label12.to(device)\n",
        "                pa = torch.stack([labels, labels2, labels3, labels4, labels5,labels6,labels7,labels8,labels9,labels10, labels11, labels12], dim = 1)\n",
        "\n",
        "                pa = torch.tensor(pa, dtype=torch.float32)\n",
        "\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # BERTに入力\n",
        "                    outputs = net(inputs)\n",
        "##############################################################################################################################################\n",
        "###################　　　　　　　ここまではおそらくあってる　　　　　　　###################################################################################\n",
        "################################################################################################################################################\n",
        "\n",
        "                    loss = criterion(outputs, pa)  # 損失を計算\n",
        "\n",
        "#第二引数の1は行。axis = 1と同じ。\n",
        "#torch.maxは最大値（テンソル）とその要素位置の２つを返しますが、その最大値を_で受けとっています。　ただ、最大値は不要なので適当な名前(_)の変数としています。\n",
        "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "\n",
        "                    # 訓練時は逆誤差伝搬\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "#10イテレーションごとの計算で、割るときの分母にはバッチサイズに12をかけておく\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            acc = (torch.sum(preds == pa.data)\n",
        "                                   ).double()/(12*batch_size)\n",
        "                            print('イテレーション {} || Loss: {:.4f} || 10iter. || 本イテレーションの正解率：{}'.format(\n",
        "                                iteration, loss.item(),  acc))\n",
        "\n",
        "                    iteration += 1\n",
        "\n",
        "                    # 損失と正解数の合計を更新\n",
        "                    epoch_loss += loss.item() * batch_size\n",
        "                    epoch_corrects += torch.sum(preds == pa.data)\n",
        "                    \n",
        "\n",
        "            # epochごとのlossと正解率\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double(\n",
        "            ) / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
        "                                                                           phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ED2Bdc1HCHt"
      },
      "source": [
        "# 学習・検証を実行する。\n",
        "num_epochs = 1\n",
        "\n",
        "net_trained = train_model(net, dataloaders_dict,\n",
        "                          criterion, optimizer, num_epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzQqmOLHHRHY"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# テストデータでの正解率を求める\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net_trained.eval()   # モデルを検証モードに\n",
        "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "# epochの正解数を記録する変数\n",
        "epoch_corrects = 0\n",
        "\n",
        "a = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
        "\n",
        "for batch in tqdm(dl_test):  # testデータのDataLoader\n",
        "    # batchはTextとLableの辞書オブジェクト\n",
        "    # GPUが使えるならGPUにデータを送る\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = batch.Text[0].to(device)  # 文章\n",
        "    labels = batch.Label.to(device)  # ラベル\n",
        "    labels2 = batch.Label2.to(device)\n",
        "    labels3 = batch.Label3.to(device)\n",
        "    labels4 = batch.Label4.to(device)\n",
        "    labels5 = batch.Label5.to(device)\n",
        "    labels6 = batch.Label6.to(device)\n",
        "    labels7 = batch.Label7.to(device)\n",
        "    labels8 = batch.Label8.to(device)\n",
        "    labels9 = batch.Label9.to(device)\n",
        "    labels10 = batch.Label10.to(device)\n",
        "    labels11 = batch.Label11.to(device)\n",
        "    labels12 = batch.Label12.to(device)\n",
        "    pa = torch.stack([labels, labels2, labels3, labels4, labels5,labels6,labels7,labels8,labels9,labels10, labels11, labels12], dim = 1)\n",
        "\n",
        "    pa = torch.tensor(pa, dtype=torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "    # 順伝搬（forward）計算\n",
        "    with torch.set_grad_enabled(False):\n",
        "\n",
        "        # BertForReviewに入力\n",
        "        outputs = net_trained(inputs)\n",
        "\n",
        "        loss = criterion(outputs, pa)  # 損失を計算\n",
        "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "        epoch_corrects += torch.sum(preds == pa.data)  # 正解数の合計を更新\n",
        "        \n",
        "        cm = confusion_matrix(pa.cpu().numpy(), preds.cpu().numpy()) # 混同行列(numpy.ndarray)の取得\n",
        "        print(\"コンフュージョンマトリックス\")\n",
        "        a = cm + a\n",
        "        print(a)\n",
        "\n",
        "\n",
        "# 正解率\n",
        "epoch_acc = epoch_corrects.double() / len(dl_test.dataset)\n",
        "\n",
        "print('テストデータ{}個での正解率：{:.4f}'.format(len(dl_test.dataset), epoch_acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}